{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "# DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "David Rose<br/>\n",
    "david.rose@berkeley.edu<br/>\n",
    "W261-1<br/>\n",
    "Week 03<br/>\n",
    "2015.09.18\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "#### HW3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "* Merge sort is an algorithm that combines two or more lists that are in sorted order individually and outputs a single list in sorted order. The algorithm compares the first elements of each input list to determine the element with the lowest (or highest) value. This value is removed from the input list and appended to the output list. This process repeats until there are no remaining input values. The algorithm runs in O(m log n) time where m in the total number of elements in the combined input lists and n is the number of input lists. Hadoop uses a merge sort when preparing data to be sent to a reduce task to provide the reducer with a list of input sorted by key.\n",
    "\n",
    "* A combiner function aggregates or otherwise combines output from a mapper function before it is sent to a reducer. It functions similarly to a reducer but with some constraints. In part the constraints are imposed because the combiner is not guaranteed to execute. Whether it does or not is a runtime decision by the Hadoop process. \n",
    "    * One constraint is that the output of the combiner must match the signature of the output of the mapper. \n",
    "    * Another is that the combiner will only operate on the output of the mapper tasks of a single node so that in a multi-node environment one combiner will not see all of the data but rather only the data that has been assigned to the map tasks on that particular node. \n",
    "    * Combiners can be very effective in reducing the amount of bandwidth required for transmitting data from a map task to a reduce task. \n",
    "    * An example usage would be in an item counting operation, whether it be words in a document or items in a shopping basket. A combiner can take the canonical map output, [key, 1], and produce for each key encountered a combined output, [key, n], where n is the sum of counts for the key.\n",
    "\n",
    "* The Hadoop shuffle is the core operation of the Hadoop framework. The shuffle is reponsible for moving data from the map task output to the reduce task input.\n",
    "    * Map side of shuffle:\n",
    "        * map output is written to a circular memory buffer\n",
    "        * when the buffer reaches a capacity threshold, data is spilled to disk\n",
    "        * before being written to spill files, data is partitioned based on the reducer targets and sorted in memory by key\n",
    "        * when map function completes, multiple spill files, if present, are merged into a single partitioned and sorted file.\n",
    "    * Reduce side of shuffle:\n",
    "        * the reducer receives notice from the job tracker that a map task is complete\n",
    "        * the reducer begins copying the sorted files from the identified map nodes\n",
    "        * the files are merged together, potentially in several rounds, maintaining the sort order\n",
    "        * the files are fed to the reduce function\n",
    "\n",
    "\n",
    "* The Apriori algorithm is used for gaining efficiency when deriving association rules. The algorithm is based on the insight that if an itemset I = {i<sub>1</sub>..i<sub>n</sub>} is frequent (i.e., occurs greater than a pre-defined threshold in the data set), then all subsets of I are also frequent. In practice this is implemented by determining the frequent itemsets of length k = 1, removing the itemsets that are not frequent, and using the remaining itemsets to construct a candidate set of k + 1 items. This process is repeated iteratively until the itemset size of interest is reached or when there are no frequent itemsets remaining. \n",
    "    * Although I can find no references to the use of the apriori algorithm in public health outbreak investigations, it could potentially be used to identify potential association rules between behaviors, events, symptoms, and diagnoses. Maybe.\n",
    "* Confidence: a measure of a rule $(X \\rightarrow Y)$ calculated as the ratio of the support of an itemset to the support of a chosen subset of the itemset, i.e., the left hand side of the rule: $$confidence(X \\rightarrow Y) = \\frac{support(X \\cup Y)}{support(X)}$$\n",
    "* Lift: a measure of a rule $(X \\rightarrow Y)$ calculated as the ratio of the confidence of the rule to the expected support of each constituent of the rule if the constituents were statistically independent: $$lift(X \\rightarrow Y) = \\frac{confidence(X \\rightarrow Y)}{support(Y)} = \\frac{support(X \\cup Y)}{support(X)support(Y)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data file into hdfs\n",
    "!hdfs dfs -put -f ProductPurchaseData.txt /in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "#### HW3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper for exploratory data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map-3.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile map-3.1.py\n",
    "#!/usr/bin/python\n",
    "''' count some things\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "random.seed()\n",
    "for line in sys.stdin:\n",
    "    # split line into tokens of items\n",
    "    tokens = line.split()\n",
    "    basketsize = len(tokens)\n",
    "    # generate a basket key\n",
    "    # this is probably overkill\n",
    "    basketkey = ''.join([str(random.random()), str(time.time())])\n",
    "    for token in tokens:\n",
    "        print('\\t'.join([token, str(basketsize), str(basketkey)]), file=sys.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Reducer for exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce-3.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce-3.1.py\n",
    "#!/usr/bin/python\n",
    "''' reducer aggregates some statistics\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "items = {}\n",
    "baskets = {}\n",
    "maxbasket = 0\n",
    "for line in sys.stdin:\n",
    "    tokens = line.split()\n",
    "    item = tokens[0]\n",
    "    basketsize = int(tokens[1])\n",
    "    basketkey = tokens[2]\n",
    "    items[item] = 1\n",
    "    # store size of each individual basket\n",
    "    baskets[basketkey] = basketsize\n",
    "    # keep running maximum of basket size\n",
    "    maxbasket = max(maxbasket, basketsize)\n",
    "# calculate some statistics\n",
    "# mean basket size\n",
    "meanbasket = sum(baskets.values())/float(len(baskets))\n",
    "# standard deviation of basket size\n",
    "std = (sum([(x - meanbasket)**2 for x in baskets.values()]) \n",
    "       / float(len(baskets)))**0.5\n",
    "# emit results\n",
    "results = ('total baskets: {}, unique items: {}, largest basket size: {},\\n'\n",
    "           + 'mean basket size: {:0.2f}, std deviation: {:0.2f}') \\\n",
    "    .format(len(baskets), len(items), maxbasket, meanbasket, std)\n",
    "print(results, file=sys.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing output directory\n",
      "Deleted /out/out-3.1\n",
      "executing yarn task\n",
      "checking output directory: Found 2 items\n",
      "-rw-r--r--   1 david supergroup          0 2015-09-21 17:50 /out/out-3.1/_SUCCESS\n",
      "-rw-r--r--   1 david supergroup        115 2015-09-21 17:50 /out/out-3.1/part-00000\n",
      "displaying results\n",
      "total baskets: 31101, unique items: 12592, largest basket size: 37,\t\n",
      "mean basket size: 12.24, std deviation: 5.03\t\n"
     ]
    }
   ],
   "source": [
    "# HW3.1. Do some exploratory data analysis of this dataset. \n",
    "# Report your findings such as number of unique products; largest \n",
    "# basket, etc. using Hadoop Map-Reduce.\n",
    "!printf \"preparing output directory\\n\"\n",
    "!hdfs dfs -rm -r -f -skipTrash /out/out-3.1\n",
    "!printf \"executing yarn task\\n\"\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \\\n",
    "-files ./map-3.1.py,./reduce-3.1.py \\\n",
    "-mapper ./map-3.1.py -reducer ./reduce-3.1.py \\\n",
    "-input /in/ProductPurchaseData.txt -output /out/out-3.1\n",
    "!printf \"checking output directory: \"\n",
    "!hdfs dfs -ls /out/out-3.1\n",
    "!printf \"displaying results\\n\"\n",
    "!hdfs dfs -cat /out/out-3.1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map-3.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile map-3.2.py\n",
    "#!/usr/bin/python\n",
    "''' map function to generate 2-itemsets\n",
    "\n",
    "    implements in-memory combiner to aggregate counts of each\n",
    "    2-itemset to reduce output file size\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "itemsets = {}\n",
    "magic = '*'\n",
    "for line in sys.stdin:\n",
    "    # split line into tokens of items\n",
    "    items = line.split()\n",
    "    for i in range(0, len(items)):\n",
    "        item1 = items[i]\n",
    "        # create fake itemset to keep count of 1-itemset\n",
    "        # for use in reduce task\n",
    "        itemset = ' '.join([item1, magic])\n",
    "        if itemset not in itemsets:\n",
    "            itemsets[itemset] = 0\n",
    "        itemsets[itemset] += 1\n",
    "        # create all possible combinations of 2-itemsets\n",
    "        # for this basket\n",
    "        for j in range(i + 1, len(items)):\n",
    "            item2 = items[j]\n",
    "            itemset = ' '.join(sorted([item1, item2]))\n",
    "            if itemset not in itemsets:\n",
    "                itemsets[itemset] = 0\n",
    "            itemsets[itemset] += 1\n",
    "for itemset in itemsets:\n",
    "    results = '\\t'.join([itemset, str(itemsets[itemset])])\n",
    "    print(results, file=sys.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce-3.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce-3.2.py\n",
    "#!/usr/bin/python\n",
    "''' reducer reads stream consisting of [2-itemset, count] pairs\n",
    "\n",
    "    utilizes order inversion pattern to enable efficient\n",
    "    stream processing\n",
    "    \n",
    "'''\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "# value for identifying left hand side of rule\n",
    "magic = '*'\n",
    "minsupport = 100\n",
    "lhscurrent = ''\n",
    "rhscurrent = ''\n",
    "lhscount = 0\n",
    "itemsetcount = 0\n",
    "# loop through input\n",
    "# when key changes, take action depending on which\n",
    "# component of the key changes\n",
    "for line in sys.stdin:\n",
    "    tokens = line.split('\\t')\n",
    "    itemset = tokens[0]\n",
    "    lhs = itemset.split()[0]\n",
    "    rhs = itemset.split()[1]\n",
    "    count = int(tokens[1])\n",
    "\n",
    "    if not rhs == rhscurrent:\n",
    "        if itemsetcount > minsupport and not rhscurrent == magic:\n",
    "            # calculate the confidence for the current itemset\n",
    "            confidence = itemsetcount / float(lhscount)\n",
    "            print('{} -> {}\\t{:0.8f}'\n",
    "                  .format(lhscurrent, rhscurrent, confidence),\n",
    "                 file=sys.stdout)\n",
    "        # reset loop values for right hand side\n",
    "        itemsetcount = 0\n",
    "        rhscurrent = rhs\n",
    "    if not lhs == lhscurrent:\n",
    "        # initialize the new lhs\n",
    "        # reset loop values for left hand side\n",
    "        lhscount = 0\n",
    "        lhscurrent = lhs\n",
    "    if rhs == magic:\n",
    "        # increment support count for left hand side\n",
    "        lhscount += count\n",
    "    else:\n",
    "        # increment support count for itemset\n",
    "        itemsetcount += count\n",
    "# process the last line\n",
    "if itemsetcount > minsupport and not rhscurrent == magic and lhscount > 0:\n",
    "    # calculate the confidence for the current itemset\n",
    "    confidence = itemsetcount / float(lhscount)\n",
    "    print('{} -> {}\\t{:0.8f}'.format(lhscurrent, rhscurrent, confidence),\n",
    "         file=sys.stdout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing output directory\n",
      "Deleted /out/out-3.2\n",
      "executing yarn task\n",
      "checking output directory: Found 2 items\n",
      "-rw-r--r--   1 david supergroup          0 2015-09-21 17:50 /out/out-3.2/_SUCCESS\n",
      "-rw-r--r--   1 david supergroup      41952 2015-09-21 17:50 /out/out-3.2/part-00000\n",
      "displaying results\n",
      "DAI93865 -> FRO40251\t1.00000000\n",
      "ELE12951 -> FRO40251\t0.99056604\n",
      "DAI88079 -> FRO40251\t0.98672566\n",
      "DAI43868 -> SNA82528\t0.97297297\n",
      "DAI23334 -> DAI62779\t0.95454545\n"
     ]
    }
   ],
   "source": [
    "# HW3.2. \n",
    "\n",
    "!printf \"preparing output directory\\n\"\n",
    "!hdfs dfs -rm -r -f -skipTrash /out/out-3.2\n",
    "!printf \"executing yarn task\\n\"\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \\\n",
    "-files ./map-3.2.py,./reduce-3.2.py \\\n",
    "-mapper ./map-3.2.py -reducer ./reduce-3.2.py \\\n",
    "-input /in/ProductPurchaseData.txt -output /out/out-3.2\n",
    "!printf \"checking output directory: \"\n",
    "!hdfs dfs -ls /out/out-3.2\n",
    "!printf \"displaying results\\n\"\n",
    "!hdfs dfs -cat /out/out-3.2/part-00000 | sort -k 4nr -k 1 2>/dev/null | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The approach outlined here relies on multiple invocations of hadoop, each with a single map and reduce function. Each invocation produces a file that will be made available through the hadoop shared cache architecture. This file will be used by the tasks in the next invocation.\n",
    "\n",
    "* Round 1\n",
    "    * Map1: \n",
    "        * emit count (itemset, 1) for all 1-itemsets\n",
    "    * Reduce1:\n",
    "        * aggregate counts for all 1-itemsets\n",
    "        * filter itemsets based on minimum support value\n",
    "        * emit [itemset, count] for all frequent itemsets\n",
    "        \n",
    "    * file(s) produced by Reduce1 are combined, if more than 1, into file frequentitems1\n",
    "    \n",
    "* Round 2\n",
    "    * Map2: \n",
    "        * read in list of frequent 1-itemsets from file frequentitems1\n",
    "        * emit count (itemset, 1) for all 2-itemsets iff the itemset contains at least one frequent 1-itemset\n",
    "    * Reduce2:\n",
    "        * aggregate counts for all 2-itemsets\n",
    "        * filter itemsets based on minimum support value\n",
    "        * emit [itemset, count] for all frequent itemsets\n",
    "\n",
    "    * file(s) produced by Reduce2 are combined, if more than 1, into file frequentitems2\n",
    "    \n",
    "* Round 3\n",
    "    * Map3:\n",
    "        * read in list of frequent 2-itemsets from file frequentitems2\n",
    "        * emit count (itemset, 1) for all 3-itemsets iff the itemset contains at least one frequent 2-itemset\n",
    "    * Reduce3\n",
    "        * aggregate counts for all 3-itemsets\n",
    "        * filter itemsets based on minimum support value\n",
    "        * read in list of frequent 2-itemsets from file frequentitems2 to make support levels for 2-itemsets available\n",
    "        * for each frequent 3-itemset (X, Y, Z) \n",
    "            * calculate confidence values for rules $((X, Y) \\rightarrow Z), ((X, Z) \\rightarrow Y), ((Y, Z) \\rightarrow X)$ using the support values for the 3-itemsets and 2-itemsets, respectively.\n",
    "                * e.g., $confidence((X, Y) \\rightarrow Z) = \\frac{support((X, Y) \\cup Z)}{support((X, Y))}$\n",
    "            * emit rule and confidence value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "utility script for validating confidence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "def confidence(itemA, itemB):\n",
    "    with open('./ProductPurchaseData.txt', 'r') as fin:\n",
    "        atotal = 0\n",
    "        btotal = 0\n",
    "        aonlytotal = 0\n",
    "        bonlytotal = 0\n",
    "        bothtotal = 0\n",
    "        for line in fin:\n",
    "            items = line.split()\n",
    "            if itemA in items:\n",
    "                atotal += 1\n",
    "            if itemB in items:\n",
    "                btotal += 1\n",
    "            if itemA in items and not itemB in items:\n",
    "                aonlytotal += 1\n",
    "            if not itemA in items and itemB in items:\n",
    "                bonlytotal += 1\n",
    "            if itemA in items and itemB in items:\n",
    "                bothtotal += 1\n",
    "    print('LHS total: {}, LHS exclusive: {}, RHS total: {}, '\n",
    "          + 'RHS exclusive: {}, both: {}' \\\n",
    "          .format(atotal, aonlytotal, btotal, bonlytotal, bothtotal))\n",
    "    print('{} -> {}, confidence: {}'\n",
    "          .format(itemA, itemB, bothtotal / float(atotal)))\n",
    "    #print('{} -> {}, confidence: {}'.format(itemB, itemA, bothtotal / float(btotal)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHS total: {}, LHS exclusive: {}, RHS total: {}, RHS exclusive: 208, both: 0\n",
      "DAI93865 -> FRO40251, confidence: 1.0\n",
      "LHS total: {}, LHS exclusive: {}, RHS total: {}, RHS exclusive: 1, both: 0\n",
      "SNA98488 -> SNA99873, confidence: 1.0\n"
     ]
    }
   ],
   "source": [
    "confidence('DAI93865', 'FRO40251')\n",
    "confidence('SNA98488', 'SNA99873')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "MIDS-W261-2015-HWK-Week01-rose.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
