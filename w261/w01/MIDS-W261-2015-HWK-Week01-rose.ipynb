{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "# DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "David Rose<br/>\n",
    "david.rose@berkeley.edu<br/>\n",
    "W261-1<br/>\n",
    "Week 01<br/>\n",
    "2015.08.31\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "#### HW1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Everyone seems to have their own definition of big data, the 3 Vs, etc. Here's another: big data is information of sufficient size and complexity to require a new and different set of tools and techniques to effectively make use of it as compared to traditional data processing. A orollary of this definition is that in the near future what is considered big data will no longer be, since the tools and techniques will have become the new normal.\n",
    "\n",
    "The human genome data is an example of big data. Genomic information will play an increasingly large role in healthcare and population health in the future. In terms of size the 1000 Genomes Project contains more than 200 TB of data, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "#### HW1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Let n be the number of polynomial regression models to be considered.\n",
    "\n",
    "Let m<sub>n</sub> be the n<sup>th</sup> model with polynomial degree n.\n",
    "\n",
    "Let j be the number of records in dataset T.\n",
    "\n",
    "Let k be the number of desired subsets of T to be used for training and testing.\n",
    "\n",
    "Divide T into k subsets containing (j/k) records in each, such that T<sub>k</sub> is the j<sup>th</sup> subset of T.\n",
    "\n",
    "For each model m<sub>p</sub> do\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "For each data subset T<sub>q</sub> do\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Divide T<sub>q</sub> into two non-overlapping subsets of equal size, T<sub>q<sup>train</sup></sub> and T<sub>q<sup>test</sup></sub>, to be used for training and testing model m<sub>p</sub>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Train model m<sub>p</sub> on data T<sub>q<sup>train</sup></sub>  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "For each tuple [x,y] in T<sub>q<sup>test</sup></sub>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Calculate m<sub>p</sub>(x); store m<sub>p</sub>(x) and y for subsequent calculations\n",
    "\n",
    "\n",
    "For each model calculate estimated average bias, variance and prediction error:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "bias<sup>2</sup> = average of the average squared difference between m<sub>p</sub>(x) and y for each T<sub>q<sup>test</sup></sub>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "variance = average difference between m<sub>p</sub>(x) and the average value of m<sub>p</sub>(x) for all datasets T<sub>q<sup>test</sup></sub>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "prediction error = bias<sup>2</sup> + variance plus a constant representing noise for each T<sub>q<sup>test</sup></sub>. The constant is ignored since it is, after all, constant.\n",
    "\n",
    "The best model is selected by determining the model with the minimal prediction error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "#### HW1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done"
     ]
    }
   ],
   "source": [
    "#HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "!printf 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper script. The same mapper is used for each subsequent exercise.\n",
    "Of note:\n",
    "* both email subject and email body are considered together\n",
    "* in addition to emitting word counts the mapper also emits email classification counts\n",
    "  * this is not consistent with functional programming, but here it's okay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "''' mapper reads name of file containing chunk of email records and a list of\n",
    "    words of interest\n",
    "\n",
    "    mapper emits counts of words, and counts of email classification\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].split()\n",
    "# regular expression to remove all punctuation\n",
    "punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        # split line into three tokens: id, classification, email contents\n",
    "        # both the email subject and the email body are included in the analysis\n",
    "        tokens = line.split('\\t', 2)\n",
    "        isspam = tokens[1]\n",
    "        # emit count of email classification, using magic word '__CLASS__'\n",
    "        if isspam == '0': # ham\n",
    "            print('__CLASS__', 1, 0)\n",
    "        else: # spam\n",
    "            print('__CLASS__', 0, 1)\n",
    "        # convert text to lower case\n",
    "        text = tokens[len(tokens) - 1].lower()\n",
    "        # remove punctuation from text\n",
    "        text = punctuation.sub('', text)\n",
    "        # split into individual words\n",
    "        words = re.findall(r\"[\\w']+\", text)\n",
    "        for word in words:\n",
    "            # only report on word if it is in the word list parameter\n",
    "            # or report on all words if parameter equals '*'\n",
    "            if word in findwords or sys.argv[2] == '*':\n",
    "                # emit the word and the classification count\n",
    "                if isspam == '0': # ham\n",
    "                    print(word, 1, 0)\n",
    "                else: # spam\n",
    "                    print(word, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Reducer script for HW1.2. Aggregates results from mapper, emits summed counts of all words processed by mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "''' reducer is provided list of temporary files containing mapper results\n",
    "\n",
    "    reducer reads each file and aggregates counts of words, them emits those counts\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "filelist = sys.argv\n",
    "words = {}\n",
    "while len(filelist) > 1: # do not use sys.argv[0]\n",
    "    with open(filelist.pop(), 'r') as cfile:\n",
    "        for line in cfile:\n",
    "            tokens = line.split()\n",
    "            word = tokens[0]\n",
    "            if word not in words.keys():\n",
    "                words[word] = 0\n",
    "            # mapper produces counts based on email classification\n",
    "            # this reducer is only interested in total counts\n",
    "            words[word] += int(tokens[1]) + int(tokens[2])\n",
    "# emit results\n",
    "for word in sorted(words.keys()):\n",
    "    # ignore counts for email classification\n",
    "    if word != '__CLASS__':\n",
    "        print('\\t'.join([word, str(words[word])]), file=sys.stderr)\n",
    "        print('\\t'.join([word, str(words[word])]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "# HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "# will determine the number of occurrences of a single, user-specified word.\n",
    "!chmod +x *.py\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducer script for HW1.3-5. Aggregates results from mapper to generate vocabulary and email classification counts.\n",
    "\n",
    "Test the results against the same data set, classifying email records and comparing the results to known classifications.\n",
    "\n",
    "**NOTE:** executing the next cell will overwrite the mapper script created in the earlier cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "''' reducer is provided a list of temporary files containing mapper results\n",
    "\n",
    "    reducer reads each file and aggregates counts of words and email classifications\n",
    "\n",
    "    reducer then applies a Naive Bayes classifier against the same data set as used\n",
    "    to buld the training parameters, classifying emal records and comparing the\n",
    "    results to known classsifications.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "# store statistics on the original list of words of interest\n",
    "keywords = {}\n",
    "# counts of each email classification\n",
    "hamcount = 0\n",
    "spamcount = 0\n",
    "# counts of words in each email classification\n",
    "spamwordcount = 0\n",
    "hamwordcount = 0\n",
    "\n",
    "filelist = sys.argv\n",
    "while len(filelist) > 1:\n",
    "    with open(filelist.pop(), 'r') as cfile:\n",
    "        for line in cfile:\n",
    "            tokens = line.split()\n",
    "            word = tokens[0]\n",
    "            # special case for count of email classification\n",
    "            if word == '__CLASS__':\n",
    "                hamcount += int(tokens[1])\n",
    "                spamcount += int(tokens[2])\n",
    "            # regular case of count of word\n",
    "            else:\n",
    "                if word not in keywords.keys():\n",
    "                    keywords[word] = [0, 0]\n",
    "                keywords[word][0] += int(tokens[1])\n",
    "                keywords[word][1] += int(tokens[2])\n",
    "                hamwordcount += int(tokens[1])\n",
    "                spamwordcount += int(tokens[2])\n",
    "# total number of unique words\n",
    "vocabcount = len(keywords)\n",
    "# total number of email records\n",
    "doccount = spamcount + hamcount\n",
    "\n",
    "# counters for determining error rate\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "# regular expression for removing punctuation\n",
    "punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "with open('enronemail_1h.txt', 'r') as cfile:\n",
    "    for line in cfile:\n",
    "        # words to be used in Naive Bayes classification\n",
    "        nbwords = {}\n",
    "        tokens = line.split('\\t', 2)\n",
    "        eid = tokens[0]\n",
    "        isspam = tokens[1]\n",
    "        # build bag of words for email record\n",
    "        text = tokens[len(tokens) - 1].lower()\n",
    "        text = punctuation.sub('', text)\n",
    "        docwords = re.findall(r\"\\w+\", text)\n",
    "        for word in docwords:\n",
    "            if word in keywords.keys():\n",
    "                if word not in nbwords:\n",
    "                    nbwords[word] = 1\n",
    "                else:\n",
    "                    nbwords[word] += 1\n",
    "\n",
    "        # calculate the probability of the email record being spam or ham\n",
    "        # natural log conversion is used to avoid floating point underflow\n",
    "\n",
    "        # start with the prior probability of a spam record\n",
    "        logpspam = math.log(spamcount / float(doccount))\n",
    "        for word in nbwords:\n",
    "            # add the probability of the word being present in this classification\n",
    "            # multiplied by the number of times the word appears in the record\n",
    "            logpspam += (nbwords[word] * \n",
    "                (math.log(keywords[word][1] + 1 / float(spamwordcount + vocabcount))))\n",
    "\n",
    "        # start with the prior probability of a ham record\n",
    "        logpham = math.log(hamcount / float(doccount))\n",
    "        for word in nbwords:\n",
    "            # add the probability of the word being present in this classification\n",
    "            # multiplied by the number of times the word appears in the record\n",
    "            logpham += (nbwords[word] * (math.log(keywords[word][0] + 1 / float(hamwordcount + vocabcount))))\n",
    "\n",
    "        # determine the classification, based on comparison of log probabilities\n",
    "        nbclass = '0' \n",
    "        if logpspam > logpham:\n",
    "            nbclass = '1'\n",
    "\n",
    "        # add some statistics\n",
    "        if isspam == nbclass:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "\n",
    "        # emit the results\n",
    "        #print('\\t'.join([eid, isspam, nbclass, str(isspam == nbclass)]), file=sys.stderr)\n",
    "        print('\\t'.join([eid, isspam, nbclass]))\n",
    "# print some statistics\n",
    "print('correct: {}, incorrect: {}, training error: {}'.format(correct, incorrect,\n",
    "    str(float(incorrect) / (correct + incorrect))), file=sys.stderr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 60, incorrect: 40, training error: 0.4\r\n"
     ]
    }
   ],
   "source": [
    "# HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "# will classify the email messages by a single, user-specified word \n",
    "# using the Naive Bayes Formulation.\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 63, incorrect: 37, training error: 0.37\r\n"
     ]
    }
   ],
   "source": [
    "# HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "# will classify the email messages by a list of one or more user-specified words.\n",
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 100, incorrect: 0, training error: 0.0\r\n"
     ]
    }
   ],
   "source": [
    "# HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "# will classify the email messages by all words present.\n",
    "!./pNaiveBayes.sh 4 \"*\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark script compares performance (in terms of error rates, not execution time) of the SciKit-Learn implementations of the Multinomial Naive Bayes algorithm and the Bernoulli Naive Bayes algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting benchmark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmark.py\n",
    "#!/Users/david/anaconda/bin/python\n",
    "from __future__ import print_function\n",
    "import re\n",
    "import string\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys\n",
    "records = []\n",
    "labels = []\n",
    "# regular expression for removing punctuation\n",
    "punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# read the input data and create separate lists for content and classification\n",
    "with open('enronemail_1h.txt', 'r') as cfile:\n",
    "    for line in cfile:\n",
    "        tokens = line.split('\\t', 2)\n",
    "        eid = tokens[0]\n",
    "        label = tokens[1]\n",
    "        # prepare text\n",
    "        text = tokens[len(tokens) - 1].lower()\n",
    "        text = punctuation.sub('', text)\n",
    "        records.append(text) # content\n",
    "        labels.append(label) # classification\n",
    "# prepare the features, using the SciKit-Learn CountVectorizer\n",
    "data = CountVectorizer().fit_transform(records)\n",
    "\n",
    "# train and test using the Multinmial Naive Bayes implemenation\n",
    "clf = MultinomialNB()\n",
    "clf.fit(data, labels)\n",
    "results = clf.predict(data)\n",
    "# measure and report training error\n",
    "incorrect = 0\n",
    "for a,b in zip(labels, results):\n",
    "    incorrect += not a == b\n",
    "print('Multinomial NB Training Error: ', str(float(incorrect) / len(results)), file=sys.stderr)\n",
    "\n",
    "# train and test using the Multinmial Naive Bayes implemenation\n",
    "clf = BernoulliNB()\n",
    "clf.fit(data, labels)\n",
    "results = clf.predict(data)\n",
    "# measure and report training error\n",
    "incorrect = 0\n",
    "for a,b in zip(labels, results):\n",
    "    incorrect += not a == b\n",
    "print('Bernoulli NB Training Error:   ', str(float(incorrect) / len(results)), file=sys.stderr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### HW1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Training Error:  0.0\n",
      "Bernoulli NB Training Error:    0.21\n",
      "HW1.5 Training Error: correct: 100, incorrect: 0, training error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# HW1.6 Benchmark your code with the Python SciKit-Learn implementation of Naive Bayes\n",
    "!chmod +x *py\n",
    "!./benchmark.py\n",
    "!printf 'HW1.5 Training Error: ' && ./pNaiveBayes.sh 4 \"*\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "    \n",
    "|Model Type|Training Error|\n",
    "|----------|--------------|\n",
    "|Multinomial NB|0.0|\n",
    "|Bernoulli NB|0.21|\n",
    "|HW1.5|0.0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "There are no differences in the results between the SciKit-Learn Multinomial Naive Bayes implementation and the HW1.5 implementation. Since both are training and testing over the same data set it is not surprising that both achieve a training error rate of 0.0.\n",
    "\n",
    "As seen in the table above, the SciKit-Learn Bernoulli Naive Bayes implementation did not perform as well as the Multinomial Naive Bayes implemenation. This can be ascribed to the fact that the Bernoulli approach uses a dichotomous value for the presence or absence of a term in an email record, whereas the Multinomial approach takes into consideration the number of times a term occurs, yielding a more accurate representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "MIDS-W261-2015-HWK-Week01-rose.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
