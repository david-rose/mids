{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "# DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "David Rose<br/>\n",
    "david.rose@berkeley.edu<br/>\n",
    "W261-1<br/>\n",
    "Week 04<br/>\n",
    "2015.09.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "#### HW4.0\n",
    "* **MRJob**: a Python API framework for accessing the Hadoop streaming capabilities. It differs from MapReduce in that it acts as a higher-level interface to MapReduce, yet utilizes the Hadoop MapReduce functionality. It provides a mechanism for creating data processing pipelines that MapReduce, on its own, cannot.\n",
    "* The ** *_final() methods** are defined in the MRJob class, and as such can be overridden by classes that extend MRJob. The ** *_final()** methods are executed when the input stream to the respective task is closed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW4.1\n",
    "* **Serialization** converts an object into a bytestream that can be used for transporting the object over a network or to and from disk storage. \n",
    "* Within Hadoop and MRJob processes data must be **serialized**, at a minimum, when it is first submitted to a map task, when it is spilled to disk, when it is submitted to a reduce task, and again when the results are written to disk.\n",
    "* **Default modes** for MRJob serialization are:\n",
    "    * INPUT_PROTOCOL = mrjob.protocol.RawValueProtocol\n",
    "    * INTERNAL_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    * OUTPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "#### HW4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_2_flatten.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_2_flatten.py\n",
    "#!/usr/bin/python\n",
    "''' \n",
    "'''\n",
    "from __future__ import print_function\n",
    "import csv\n",
    "import sys\n",
    "with open(sys.argv[1], 'rb') as fin, open(sys.argv[2], 'w') as fout:\n",
    "    csvreader = csv.reader(fin, delimiter = ',', quotechar = '\"')\n",
    "    visitorid = ''\n",
    "    for line in csvreader:\n",
    "        if line[0] == 'C':\n",
    "            visitorid = line[2]\n",
    "            continue\n",
    "        if line[0] == 'V':\n",
    "            line.append(visitorid)\n",
    "        print(','.join(line), file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine the page id and visitor id onto a single line for\n",
    "# subsequent processing\n",
    "!python hw_4_2_flatten.py anonymous-msweb.data flattened.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_3_mrjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_3_mrjob.py\n",
    "''' count and list the top five most frequently visited pages\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class FrequentPages(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        ''' enumerate page visits\n",
    "        '''\n",
    "        row = line.split(',')\n",
    "        if row[0] == 'V':\n",
    "            yield row[1], 1\n",
    "    \n",
    "    def combiner(self, page, i):\n",
    "        ''' combine local results\n",
    "        '''\n",
    "        yield page, sum(i)\n",
    "\n",
    "    # track top five frequent pages in sorted order\n",
    "    topfive = [['',0]]\n",
    "    \n",
    "    def inserttopfive(self, page, total):\n",
    "        ''' data structure and logic to maintain list of\n",
    "            top five most frequent pages.\n",
    "            This operation is performed here in the reducer\n",
    "            and again in the driver to capture output from\n",
    "            multiple reducers\n",
    "        '''\n",
    "        for j in range(0, len(self.topfive)):\n",
    "            if total > self.topfive[j][1]:\n",
    "                self.topfive.insert(j, (page, total))\n",
    "                if len(self.topfive) > 5:\n",
    "                    self.topfive.pop()\n",
    "                break\n",
    "    \n",
    "    def reducer(self, page, i):\n",
    "        ''' sum and sort page counts\n",
    "        '''\n",
    "        total = sum(i)\n",
    "        self.inserttopfive(page, total)\n",
    "        \n",
    "    def reducer_final(self):\n",
    "        ''' emit results\n",
    "        '''\n",
    "        for i in range(0, len(self.topfive)):\n",
    "            yield(self.topfive[i][0], self.topfive[i][1])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FrequentPages.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_3_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_3_driver.py\n",
    "from __future__ import print_function\n",
    "from mrjob import util\n",
    "import sys\n",
    "from hw_4_3_mrjob import FrequentPages\n",
    "util.log_to_null() # to suppress a 'no handler found' message\n",
    "\n",
    "# list for storing most frequent pages\n",
    "# we do this step here since multiple reducer tasks may run and their\n",
    "# combined output needs to be processed\n",
    "topfive = [['',0]]\n",
    "def inserttopfive(page, total):\n",
    "    for j in range(0, len(topfive)):\n",
    "        if total > topfive[j][1]:\n",
    "            topfive.insert(j, (page, total))\n",
    "            if len(topfive) > 5:\n",
    "                topfive.pop()\n",
    "                break\n",
    "\n",
    "mr_job = FrequentPages(args=sys.argv[1:])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        page, total = line.split()\n",
    "        inserttopfive(page, int(total))\n",
    "for i in range(0, len(topfive)):\n",
    "    print('page: {}, visits: {}'.format(topfive[i][0], \n",
    "                                        topfive[i][1]), file=sys.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: \"1008\", visits: 10836\r\n",
      "page: \"1034\", visits: 9383\r\n",
      "page: \"1004\", visits: 8463\r\n",
      "page: \"1018\", visits: 5330\r\n",
      "page: \"1017\", visits: 5108\r\n"
     ]
    }
   ],
   "source": [
    "# HW 4.3: Find the 5 most frequently visited pages using mrjob from the output of 4.2\n",
    "!python hw_4_3_driver.py flattened.data --strict-protocols -r local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_4_mrjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_4_mrjob.py\n",
    "''' count the number of page-visitor combinations and\n",
    "    for each page list the most frequent visitors\n",
    "    \n",
    "    the data does not effectively support this operation since it\n",
    "    only lists unique page visits, therefore every visitor\n",
    "    will show up as having visited once, and therefore every visitor\n",
    "    is the most frequent visitor\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class FrequentVisitors(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        ''' enumerate page visitors\n",
    "        '''\n",
    "        row = line.split(',')\n",
    "        if row[0] == 'V':\n",
    "            # page ID, visitor ID\n",
    "            yield row[1], row[3]\n",
    "    \n",
    "    # data structures to manage reducer logic\n",
    "    visitors = {}\n",
    "    currentpage = ''\n",
    "        \n",
    "    def reducer(self, page, visitor):\n",
    "        ''' sum page visitor counts\n",
    "        '''\n",
    "        if not page == self.currentpage:\n",
    "            ''' page id has changed in the stream, so process and emit\n",
    "                the information for the current page\n",
    "            '''\n",
    "            if len(self.visitors) > 0:\n",
    "                frequentv = []\n",
    "                maxv = 0\n",
    "                for v in self.visitors:\n",
    "                    if self.visitors[v] > maxv:\n",
    "                        frequentv = [v]\n",
    "                        maxv = self.visitors[v]\n",
    "                    elif self.visitors[v] == maxv:\n",
    "                        frequentv.append(v)\n",
    "                # emit results\n",
    "                for v in frequentv:\n",
    "                    yield self.currentpage, v                \n",
    "            # reset counters\n",
    "            self.visitors = {}\n",
    "            self.currentpage = page\n",
    "        for v in visitor:\n",
    "            if not v in self.visitors:\n",
    "                self.visitors[v] = 0\n",
    "            self.visitors[v] += 1\n",
    "    \n",
    "    # process any remaining values after stream closes\n",
    "    def reducer_final(self):\n",
    "        if len(self.visitors) > 0:\n",
    "            frequentv = []\n",
    "            maxv = 0\n",
    "            for v in self.visitors:\n",
    "                if self.visitors[v] > maxv:\n",
    "                    frequentv = [v]\n",
    "                elif self.visitors[v] == maxv:\n",
    "                    frequentv.append(v)\n",
    "            for v in frequentv:\n",
    "                yield self.currentpage, v                \n",
    "         \n",
    "if __name__ == '__main__':\n",
    "    FrequentVisitors.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_4_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_4_driver.py\n",
    "import csv\n",
    "from mrjob import util\n",
    "import sys\n",
    "from hw_4_4_mrjob import FrequentVisitors\n",
    "\n",
    "util.log_to_null() # to suppress a 'no handler found' message\n",
    "\n",
    "# construct list of page ids and urls to satisfy the requirement\n",
    "# this is essentially a join; it makes better sense to do this in\n",
    "# the driver as doing so in hadoop offers no advantages and incurs\n",
    "# additional network overhead; mrjob output is parsed and augmented with\n",
    "# the url information\n",
    "pageinfo = {}\n",
    "# read in the page attributes including the url; this could also have\n",
    "# been preprocessed so that the page attributes were in their own file\n",
    "with open('flattened.data', 'rb') as fin:\n",
    "    csvreader = csv.reader(fin, delimiter = ',', quotechar = '\"')\n",
    "    for line in csvreader:\n",
    "        if line[0] == 'A':\n",
    "            pageid = line[1]\n",
    "            url = line[4]\n",
    "            pageinfo[pageid] = url\n",
    "\n",
    "mr_job = FrequentVisitors(args=sys.argv[1:])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        page, visitor = line.replace('\"', '').split()\n",
    "        print pageinfo[page], page, visitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/regwiz 1000 10001\n",
      "/regwiz 1000 10010\n",
      "/regwiz 1000 10039\n",
      "/regwiz 1000 10073\n",
      "/regwiz 1000 10087\n",
      "/regwiz 1000 10101\n",
      "/regwiz 1000 10132\n",
      "/regwiz 1000 10141\n",
      "/regwiz 1000 10154\n",
      "/regwiz 1000 10162\n",
      "/regwiz 1000 10166\n",
      "/regwiz 1000 10201\n",
      "/regwiz 1000 10218\n",
      "/regwiz 1000 10220\n",
      "/regwiz 1000 10324\n",
      "/regwiz 1000 10348\n",
      "/regwiz 1000 10376\n",
      "/regwiz 1000 10384\n",
      "/regwiz 1000 10409\n",
      "/regwiz 1000 10429\n",
      "/regwiz 1000 10454\n",
      "/regwiz 1000 10457\n",
      "/regwiz 1000 10471\n",
      "/regwiz 1000 10497\n",
      "/regwiz 1000 10511\n",
      "/regwiz 1000 10520\n",
      "/regwiz 1000 10541\n",
      "/regwiz 1000 10564\n",
      "/regwiz 1000 10599\n",
      "/regwiz 1000 10752\n",
      "/regwiz 1000 10756\n",
      "/regwiz 1000 10861\n",
      "/regwiz 1000 10935\n",
      "/regwiz 1000 10943\n",
      "/regwiz 1000 10969\n",
      "/regwiz 1000 11027\n",
      "/regwiz 1000 11050\n",
      "/regwiz 1000 11410\n",
      "/regwiz 1000 11429\n",
      "/regwiz 1000 11440\n",
      "/regwiz 1000 11490\n",
      "/regwiz 1000 11501\n",
      "/regwiz 1000 11528\n",
      "/regwiz 1000 11539\n",
      "/regwiz 1000 11544\n",
      "/regwiz 1000 11685\n",
      "/regwiz 1000 11695\n",
      "/regwiz 1000 11723\n",
      "/regwiz 1000 11766\n",
      "/regwiz 1000 11774\n",
      "/peru 1258 28356\n",
      "/peru 1258 30514\n",
      "/controls 1259 21424\n",
      "/trial 1260 21894\n",
      "/diyguide 1261 22485\n",
      "/diyguide 1261 36145\n",
      "/chile 1262 24951\n",
      "/chile 1262 25479\n",
      "/chile 1262 35732\n",
      "/chile 1262 37425\n",
      "/services 1263 26122\n",
      "/services 1263 27503\n",
      "/se_partners 1264 26801\n",
      "/se_partners 1264 27893\n",
      "/se_partners 1264 31613\n",
      "/se_partners 1264 40427\n",
      "/ssafesupport 1265 26811\n",
      "/ssafesupport 1265 35353\n",
      "/ssafesupport 1265 39038\n",
      "/licenses 1266 26815\n",
      "/licenses 1266 35105\n",
      "/caribbean 1267 27482\n",
      "/caribbean 1267 28325\n",
      "/caribbean 1267 29886\n",
      "/caribbean 1267 39669\n",
      "/caribbean 1267 42071\n",
      "/javascript 1268 27503\n",
      "/business 1269 28044\n",
      "/business 1269 41054\n",
      "/developr 1270 28493\n",
      "/mdsn 1271 28493\n",
      "/softlib 1272 28493\n",
      "/mdn 1273 28493\n",
      "/pdc 1274 28493\n",
      "/security. 1275 28903\n",
      "/vtestsupport 1276 29654\n",
      "/vtestsupport 1276 37027\n",
      "/vtestsupport 1276 40810\n",
      "/stream 1277 30111\n",
      "/hed 1278 30460\n",
      "/hed 1278 41317\n",
      "/msgolf 1279 31062\n",
      "/music 1280 33424\n",
      "/music 1280 41643\n",
      "/intellimouse 1281 37099\n",
      "/home 1282 39877\n",
      "/home 1282 41244\n",
      "/cinemania 1283 41033\n",
      "/partner 1284 41108\n",
      "/train_cert 1295 41207\n"
     ]
    }
   ],
   "source": [
    "# HW 4.4: Find the most frequent visitor of each page using \n",
    "# mrjob and the output of 4.2\n",
    "# In this output please include the webpage URL, webpageID \n",
    "# and Visitor ID.\n",
    "!python hw_4_4_driver.py flattened.data --strict-protocols -r local | sort -k2n > mrjob_4_4_output\n",
    "!head -n50 mrjob_4_4_output\n",
    "!tail -n50 mrjob_4_4_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "#### HW4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1724608, 61819567, \"{'1': 91, '0': 752, '3': 103, '2': 54}\")\n"
     ]
    }
   ],
   "source": [
    "''' utility script to get some statistics on the data set\n",
    "'''\n",
    "with open('topUsers_Apr-Jul_2014_1000-words.txt', 'rb') as fin:\n",
    "    max = 0\n",
    "    total = 0\n",
    "    linecount = 0\n",
    "    classes = {}\n",
    "    for line in fin:\n",
    "        row = line.split(',')\n",
    "        count = int(row[2])\n",
    "        clazz = row[1]\n",
    "        if count > max: max = count\n",
    "        total += count\n",
    "        linecount += 1\n",
    "        if not clazz in classes:\n",
    "            classes[clazz] = 0\n",
    "        classes[clazz] += 1\n",
    "print(linecount, max, total, str(classes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.409909665665\n"
     ]
    }
   ],
   "source": [
    "''' utility script to get some statistics on the data set\n",
    "    look for the maximum word ratio after the data is normalized\n",
    "'''\n",
    "with open('topUsers_Apr-Jul_2014_1000-words.txt', 'rb') as fin:\n",
    "    max = 0.0\n",
    "    for line in fin:\n",
    "        row = line.split(',')\n",
    "        count = int(row[2])\n",
    "        for i in range(3, len(row)):\n",
    "            p = float(row[i]) / count\n",
    "            if p > max:\n",
    "                max = p\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_5_preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_5_preprocess.py\n",
    "''' preprocess the data set, normalizing the word counts\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "with open(sys.argv[1], 'rb') as fin, open(sys.argv[2], 'w') as fout:\n",
    "    for line in fin:\n",
    "        row = map(int, line.split(','))\n",
    "        userid = row[0]\n",
    "        code = row[1]\n",
    "        total = row[2]\n",
    "        for j in range(2, len(row)):\n",
    "            row[j] = float(row[j]) / total\n",
    "        print('{}'.format(row[0]), file = fout, end = '')\n",
    "        for j in range(1, 3):\n",
    "            print(',{}'.format(row[j]), file = fout, end = '')\n",
    "        for j in range(3, len(row)):\n",
    "            print(',{:0.8f}'.format(row[j]), file = fout, end = '')\n",
    "        print('', file = fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python hw_4_5_preprocess.py topUsers_Apr-Jul_2014_1000-words.txt normalized.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_5_mrjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_5_mrjob.py\n",
    "''' map/reduce approach to determining stable centroids using\n",
    "    a K-means algorithm    \n",
    "'''\n",
    "from __future__ import print_function\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import sys\n",
    "\n",
    "# Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    sumofsquares = list(diffsq.sum(axis = 1))\n",
    "    minindex = argmin(sumofsquares)\n",
    "    return minindex\n",
    "\n",
    "# Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new, T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if i > T:\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "\n",
    "    centroid_points=[]\n",
    "    k = -1\n",
    "    CENTROIDFILE = '/tmp/centroids.txt'\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, \n",
    "                   mapper=self.mapper,\n",
    "                   combiner = self.combiner,\n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "    # load initial centroids\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points=[]\n",
    "        with open(self.CENTROIDFILE, 'rb') as fin:\n",
    "            header = True\n",
    "            for line in fin:\n",
    "                if header:\n",
    "                    self.k = int(line.strip())\n",
    "                    header = False\n",
    "                else:\n",
    "                    self.centroid_points.append(map(float,line.strip().split(',')))\n",
    "        # print the value of k back to the cache file\n",
    "        with open(self.CENTROIDFILE, 'w') as fout:\n",
    "            print('{}'.format(self.k), file = fout)\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')[3:]))\n",
    "        centroid = MinDist(D, self.centroid_points)\n",
    "        yield centroid, (D, 1)\n",
    "    \n",
    "    # aggregate data points locally\n",
    "    def combiner(self, centroid, inputdata):\n",
    "        count = 0\n",
    "        bucket = [0] * 1000\n",
    "        for data, n in inputdata:\n",
    "            count += n\n",
    "            data = map(float, data)\n",
    "            for j in range(0, len(data)):\n",
    "                bucket[j] += data[j]\n",
    "        yield centroid, (bucket, count)\n",
    "\n",
    "   \n",
    "    # aggregate values for each centroid, then recalculate centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        with open(self.CENTROIDFILE, 'rb') as fin:\n",
    "            self.k = int(fin.readline().strip())\n",
    "        num = [0] * self.k \n",
    "        for i in range(self.k):\n",
    "            centroids.append([0.0]*1000)\n",
    "        for data, n in inputdata:\n",
    "            num[idx] += n\n",
    "            data = map(float, data)\n",
    "            for j in range(0, len(data)):\n",
    "                centroids[idx][j] += data[j]\n",
    "        for j in range(0, len(centroids[idx])):\n",
    "            centroids[idx][j] = centroids[idx][j] / num[idx]\n",
    "        with open(self.CENTROIDFILE, 'a') as fout:\n",
    "            print(','.join(str(i) for i in centroids[idx]), file = fout)\n",
    "        yield idx, (centroids[idx])\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_5_mrjob_labeler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_5_mrjob_labeler.py\n",
    "''' assigns centroid labels to data points\n",
    "\n",
    "    this step is done separately from the centroid calculations\n",
    "    to simplify the data processing\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "from hw_4_5_mrjob import MinDist\n",
    "\n",
    "class MRLabeler(MRJob):\n",
    "\n",
    "    centroid_points=[]\n",
    "    k = -1\n",
    "    CENTROIDFILE = '/tmp/centroids.txt'\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, \n",
    "                   mapper=self.mapper,\n",
    "                   combiner = self.combiner,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points=[]\n",
    "        with open(self.CENTROIDFILE, 'rb') as fin:\n",
    "            header = True\n",
    "            for line in fin:\n",
    "                if header:\n",
    "                    self.k = int(line.strip())\n",
    "                    header = False\n",
    "                else:\n",
    "                    self.centroid_points.append(map(float,line.strip().split(',')))\n",
    "        \n",
    "    # determine the closest centroid for each data point\n",
    "    def mapper(self, _, line):\n",
    "        row = line.strip().split(',')\n",
    "        label = int(row[1])\n",
    "        D = (map(float,row[3:]))\n",
    "        centroid = MinDist(D, self.centroid_points)\n",
    "        yield centroid, (label, 1)\n",
    "    \n",
    "    # combine sum of data points locally\n",
    "    def combiner(self, centroid, inputdata):\n",
    "        counts = [0, 0, 0, 0]\n",
    "        for label, n in inputdata:\n",
    "            counts[label] += n\n",
    "        for i in range(len(counts)):\n",
    "            yield centroid, (i, counts[i])\n",
    "\n",
    "   \n",
    "    # sum the counts for centroids and classes\n",
    "    currentcentroid = ''\n",
    "    counts = []\n",
    "    def reducer(self, centroid, inputdata): \n",
    "        if not centroid == self.currentcentroid:\n",
    "            for i in range(len(self.counts)):\n",
    "                yield self.currentcentroid, (i, self.counts[i])\n",
    "            self.counts = [0, 0, 0, 0]\n",
    "            self.currentcentroid = centroid\n",
    "        for label, n in inputdata:\n",
    "            self.counts[label] += n\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        for i in range(len(self.counts)):\n",
    "            yield self.currentcentroid, (i, self.counts[i])\n",
    "        \n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_4_5_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_4_5_driver.py\n",
    "''' driver script for Kmeans job\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from numpy import random\n",
    "from hw_4_5_mrjob import MRKmeans, stop_criterion\n",
    "from hw_4_5_mrjob_labeler import MRLabeler\n",
    "from mrjob import util\n",
    "import sys\n",
    "\n",
    "util.log_to_null() # to suppress a 'no handler found' message\n",
    "CENTROIDFILE = '/tmp/centroids.txt'\n",
    "\n",
    "def countlabels():\n",
    "    ''' count the number of each classification as labeled in the\n",
    "        original data set\n",
    "    '''\n",
    "    with open('topUsers_Apr-Jul_2014_1000-words.txt', 'rb') as f:\n",
    "        labels = {}\n",
    "        for line in f:\n",
    "            row = line.split(',')\n",
    "            label = row[1]\n",
    "            if not label in labels:\n",
    "                labels[label] = 0\n",
    "            labels[label] += 1\n",
    "    return labels\n",
    "\n",
    "def init_centroids_random_internal(k):\n",
    "    ''' select initial centroids by choosing data points\n",
    "        randomly from the data set\n",
    "    '''\n",
    "    randoms = sorted(random.randint(0, 1000, size = k))\n",
    "    centroids = []\n",
    "    with open('topUsers_Apr-Jul_2014_1000-words.txt', 'rb') as f:\n",
    "        lineno = 0\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if lineno in randoms:\n",
    "                row = line.strip().split(',')\n",
    "                data = map(float, row[3:])\n",
    "                # normalize the values\n",
    "                data = [i / float(row[2]) for i in data]\n",
    "                centroids.append(data)\n",
    "                count += 1\n",
    "                if count == k:\n",
    "                    break\n",
    "            lineno += 1\n",
    "    with open(CENTROIDFILE, 'w') as f:\n",
    "        print('{}'.format(k), file = f)\n",
    "        for tuple in centroids:\n",
    "            print(','.join(str(i) for i in tuple), file = f)\n",
    "    return centroids\n",
    "\n",
    "def init_centroids_random_external(k):\n",
    "    ''' create initial centroids by generating random values\n",
    "    '''\n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        centroid = []\n",
    "        for j in range(1000):\n",
    "            centroid.append(random.uniform(0.0, .5))\n",
    "        centroids.append(centroid)\n",
    "    with open(CENTROIDFILE, 'w') as fout:\n",
    "        print('{}'.format(k), file = fout)\n",
    "        for tuple in centroids:\n",
    "            print(','.join(str(i) for i in tuple), file = fout)\n",
    "    return centroids\n",
    "\n",
    "def init_centroids_perturbed(k):\n",
    "    ''' create initial centroids by using aggregated values and\n",
    "        perturbing them with random noise\n",
    "    '''\n",
    "    centroids = []\n",
    "    with open('topUsers_Apr-Jul_2014_1000-words_summaries.txt', 'rb') as f:\n",
    "        for line in f:\n",
    "            row = line.strip().split(',')\n",
    "            if row[0] == 'ALL_CODES':\n",
    "                data = map(float, row[3:])\n",
    "                # normalize the values\n",
    "                data = [i / float(row[2]) for i in data]\n",
    "                for i in range(k):\n",
    "                    centroid = []\n",
    "                    for j in range(len(data)):\n",
    "                        # modify each value with random number in the\n",
    "                        # range of +- the value; avoids a small number being modified\n",
    "                        # by a large number\n",
    "                        centroid.append(data[j] + random.uniform(-1 * data[j], data[j]))\n",
    "                    centroids.append(centroid)\n",
    "                break\n",
    "    with open(CENTROIDFILE, 'w') as f:\n",
    "        print('{}'.format(k), file = f)\n",
    "        for tuple in centroids:\n",
    "            print(','.join(str(i) for i in tuple), file = f)\n",
    "    return centroids\n",
    "\n",
    "def init_centroids_trained(k):\n",
    "    ''' create initial centroids by choosing the class-specific aggregate values\n",
    "    '''\n",
    "    centroids = []\n",
    "    with open('topUsers_Apr-Jul_2014_1000-words_summaries.txt', 'rb') as f:\n",
    "        for line in f:\n",
    "            row = line.strip().split(',')\n",
    "            if row[0] == 'CODE':\n",
    "                code = row[1]\n",
    "                total = int(row[2])\n",
    "                data = map(float, row[3:])\n",
    "                # normalize the values\n",
    "                data = [i / total for i in data]\n",
    "                centroids.append(data)\n",
    "    with open(CENTROIDFILE, 'w') as f:\n",
    "        print('{}'.format(k), file = f)\n",
    "        for tuple in centroids:\n",
    "            print(','.join(str(i) for i in tuple), file = f)\n",
    "    return centroids\n",
    "\n",
    "def getpurity(clusters):\n",
    "    majority = 0\n",
    "    for c in clusters:\n",
    "        counts = map(int, clusters[c]);\n",
    "        majority += max(counts)\n",
    "    return majority / float(1000)\n",
    "    \n",
    "def go(centroid_points):\n",
    "    ''' submit the centroid calculation job;\n",
    "        follow that with the data labeling job\n",
    "    '''\n",
    "    mr_job = MRKmeans(args = ['normalized.txt'])\n",
    "    iteration = 0\n",
    "    while(1):\n",
    "        # save previous centroids to check convergency\n",
    "        centroid_points_old = centroid_points[:]\n",
    "        with mr_job.make_runner() as runner: \n",
    "            runner.run()\n",
    "            # capture reducer output\n",
    "            for line in runner.stream_output():\n",
    "                key,value =  mr_job.parse_output_line(line)\n",
    "                centroid_points[key] = value\n",
    "        iteration += 1\n",
    "        if stop_criterion(centroid_points_old, centroid_points, 0.001):\n",
    "            break\n",
    "    print('centroids converged: {} iterations'.format(iteration), file = sys.stderr)\n",
    "    mr_job = MRLabeler(args = ['normalized.txt'])\n",
    "    labels = countlabels()\n",
    "    clusters = {}\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            centroid, value =  mr_job.parse_output_line(line)\n",
    "            label = int(value[0])\n",
    "            count = int(value[1])\n",
    "            if not centroid in clusters:\n",
    "                clusters[centroid] = [0,0,0,0]\n",
    "            clusters[centroid][label] += count\n",
    "        for c in sorted(clusters):\n",
    "            results = map(float, clusters[c])\n",
    "            print('centroid {}: label 0: {:.4f} label 1: {:.4f} label 2: {:.4f} label 3: {:.4f}'\n",
    "                  .format(c, results[0] / labels['0'], results[1] / labels['1'], \n",
    "                          results[2] / labels['2'], results[3] / labels['3'])\n",
    "                  )\n",
    "            # print the actual counts\n",
    "#             print('centroid {}: counts: {}'.format(c, clusters[c]), \n",
    "#                   file = sys.stderr)\n",
    "        print('purity: {:.4f}'.format(getpurity(clusters)))\n",
    "        print('', file = sys.stderr)\n",
    "\n",
    "print('k = 4, random initialization, v1', file = sys.stderr)\n",
    "go(init_centroids_random_internal(4))\n",
    "print('k = 4, random initialization, v2', file = sys.stderr)\n",
    "go(init_centroids_random_external(4))\n",
    "print('k = 2, perturbed initialization', file = sys.stderr)\n",
    "go(init_centroids_perturbed(2))\n",
    "print('k = 4, perturbed initialization', file = sys.stderr)\n",
    "go(init_centroids_perturbed(4))\n",
    "print('k = 4, trained initialization', file = sys.stderr)\n",
    "go(init_centroids_trained(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4, random initialization, v1\n",
      "centroids converged: 9 iterations\n",
      "centroid 0: label 0: 0.0000 label 1: 0.1868 label 2: 0.2778 label 3: 0.0388\n",
      "centroid 1: label 0: 0.0000 label 1: 0.5604 label 2: 0.0000 label 3: 0.0000\n",
      "centroid 2: label 0: 0.9987 label 1: 0.0330 label 2: 0.0370 label 3: 0.9612\n",
      "centroid 3: label 0: 0.0013 label 1: 0.2198 label 2: 0.6852 label 3: 0.0000\n",
      "purity: 0.8560\n",
      "\n",
      "k = 4, random initialization, v2\n",
      "centroids converged: 3 iterations\n",
      "centroid 0: label 0: 1.0000 label 1: 1.0000 label 2: 1.0000 label 3: 1.0000\n",
      "purity: 0.7520\n",
      "\n",
      "k = 2, perturbed initialization\n",
      "centroids converged: 4 iterations\n",
      "centroid 0: label 0: 0.9987 label 1: 0.0330 label 2: 0.2593 label 3: 0.9612\n",
      "centroid 1: label 0: 0.0013 label 1: 0.9670 label 2: 0.7407 label 3: 0.0388\n",
      "purity: 0.8390\n",
      "\n",
      "k = 4, perturbed initialization\n",
      "centroids converged: 4 iterations\n",
      "centroid 0: label 0: 0.0000 label 1: 0.0000 label 2: 0.0370 label 3: 0.0000\n",
      "centroid 1: label 0: 0.8816 label 1: 0.0110 label 2: 0.0000 label 3: 0.6214\n",
      "centroid 2: label 0: 0.0013 label 1: 0.9670 label 2: 0.7037 label 3: 0.0388\n",
      "centroid 3: label 0: 0.1170 label 1: 0.0220 label 2: 0.2593 label 3: 0.3398\n",
      "purity: 0.8410\n",
      "\n",
      "k = 4, trained initialization\n",
      "centroids converged: 5 iterations\n",
      "centroid 0: label 0: 0.9960 label 1: 0.0330 label 2: 0.2593 label 3: 0.3689\n",
      "centroid 1: label 0: 0.0000 label 1: 0.5604 label 2: 0.0000 label 3: 0.0000\n",
      "centroid 2: label 0: 0.0013 label 1: 0.4066 label 2: 0.7407 label 3: 0.0388\n",
      "centroid 3: label 0: 0.0027 label 1: 0.0000 label 2: 0.0000 label 3: 0.5922\n",
      "purity: 0.9010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python hw_4_5_driver.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Discussion**: centroid initialization has a significant impact on the results. The purity scores of the five runs range from 0.75 to 0.90. Not surprisingly the best results came from the centroids initialized with the class-specific aggregate data, and the worst results came from using randomized synthetic data. Ideally the results would suggest a diagonal of populated cells with all others being zero. The final scenario above comes closest, but there is still room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "MIDS-W261-2015-HWK-Week01-rose.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
