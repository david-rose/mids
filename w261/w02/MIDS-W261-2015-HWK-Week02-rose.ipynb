{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "# DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "David Rose<br/>\n",
    "david.rose@berkeley.edu<br/>\n",
    "W261-1<br/>\n",
    "Week 02<br/>\n",
    "2015.09.11\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "#### HW2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Race conditions occur when the execution of separate processes is dependent on the timing of execution of statements within those processes. Problems can occur, often non-deterministically, when the behavior of two or more threads interferes with the intended sequence, resulting in unanticipated behavior. Often this occurs when separate processes require the same resource. Example: updating a global variable. Process A reads the value of variable v, then process B reads the value of v. A increments v and writes the new value. B increments v and writes the new value. The result is that the update by process A has been overwritten by process B. Both processes have executed properly, yet the results are incorrect.\n",
    "* MapReduce is a data processing concept with two main parts. The map component takes raw input and converts each element of the input into an output value structured as a key-value pair. The reduce component aggregates the output of the map, typically based on the key, and may perform additional calculations on the result.\n",
    "* MapReduce differs from Hadoop in that MapReduce is a subset of the Hadoop system, which also includes a file system, HDFS, and other infrastructure. \n",
    "* Hadoop is based on the MapReduce concept, which in turn is based on, but does not strictly adhere to, the concept of functional programming. For a simple example please see exercise HW2.1, below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "#### HW2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate-2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%file generate-2.1.py\n",
    "#!/usr/bin/python\n",
    "''' generate a list of key-value pairs where the key is a random integer\n",
    "    and the value is an empty string\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from random import randint\n",
    "minrange = 0\n",
    "maxrange = 99\n",
    "randcount = 10000\n",
    "with open('randomnumbers.txt', 'w') as fout:\n",
    "    for i in range(0, randcount):\n",
    "        print(' '.join([str(randint(minrange, maxrange)), '']), file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate the file of random numbers\n",
    "!chmod +x *.py\n",
    "!./generate-2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map-2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%file map-2.1.py\n",
    "#!/usr/bin/python\n",
    "''' map function copies stdin to stdout '''\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print(line.strip(), '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce-2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%file reduce-2.1.py\n",
    "#!/usr/bin/python\n",
    "''' reduce function copies stdin to stdout '''\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading random number file\n",
      "preparing output directory\n",
      "Deleted /out/out-2.1\n",
      "executing yarn task\n",
      "checking output directory: Found 2 items\n",
      "-rw-r--r--   1 david supergroup          0 2015-09-14 18:52 /out/out-2.1/_SUCCESS\n",
      "-rw-r--r--   1 david supergroup      39032 2015-09-14 18:52 /out/out-2.1/part-00000\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "0\t\n",
      "cat: Unable to write to output stream.\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n",
      "99\t\n"
     ]
    }
   ],
   "source": [
    "# HW2.1. Sort in Hadoop MapReduce\n",
    "!printf \"loading random number file\\n\"\n",
    "!hdfs dfs -put -f randomnumbers.txt /in\n",
    "!printf \"preparing output directory\\n\"\n",
    "!hdfs dfs -rm -r -f -skipTrash /out/out-2.1\n",
    "!printf \"executing yarn task\\n\"\n",
    "# specify comparator and option for sorting numerically\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.text.key.comparator.options=-n \\\n",
    "    -files ./map-2.1.py,./reduce-2.1.py \\\n",
    "    -mapper ./map-2.1.py -reducer ./reduce-2.1.py \\\n",
    "    -input /in/randomnumbers.txt -output /out/out-2.1\n",
    "!printf \"checking output directory: \"\n",
    "!hdfs dfs -ls /out/out-2.1\n",
    "!hdfs dfs -cat /out/out-2.1/part-00000 | head -n 20\n",
    "!hdfs dfs -cat /out/out-2.1/part-00000 | tail -n 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: this implementation of a sort is trivial to the extreme, as it relies entirely on the sort capability of Hadoop. Both the map and reduce steps simply copy stdin to stdout. The actual sorting occurs after Hadoop captures the map output and before it presents it as input to the reducer.\n",
    "\n",
    "If there were > 1 reducer, then additional work would be required. Each reducer would produce a non-overlapping set of sorted key-value pairs (where the value is null in this case). The final step would require concatenating the output of each of the reducers while paying attention to their ordering so that the final output would be a single contiguous ordered list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mapper script. This mapper is used for each subsequent exercise.\n",
    "* in addition to emitting word counts the mapper also emits email classification counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile map.py\n",
    "#!/usr/bin/python\n",
    "''' mapper reads from stdin, emits counts of words, and \n",
    "    counts of email classifications\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# regular expression to remove all punctuation\n",
    "punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# words to be used in analysis\n",
    "# normalize parameters same as input\n",
    "findwords = punctuation.sub('', sys.argv[1]).lower().split()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # split line into three tokens: id, classification, email contents\n",
    "    # both the email subject and the email body are included in the analysis\n",
    "    tokens = line.split('\\t', 2)\n",
    "    isspam = tokens[1]\n",
    "    # emit count of email classification, using magic word '__CLASS__'\n",
    "    if isspam == '0': # ham\n",
    "        print('__CLASS__', 1, 0)\n",
    "    else: # spam\n",
    "        print('__CLASS__', 0, 1)\n",
    "    # convert text to lower case\n",
    "    text = tokens[len(tokens) - 1].lower()\n",
    "    # remove punctuation from text\n",
    "    text = punctuation.sub('', text)\n",
    "    # split into individual words\n",
    "    words = re.findall(r\"[\\w']+\", text)\n",
    "    for word in words:\n",
    "        # only report on word if it is in the word list parameter\n",
    "        # or report on all words if parameter equals '*'\n",
    "        if word in findwords or sys.argv[1] == '*':\n",
    "            # emit the word and the classification count\n",
    "            if isspam == '0': # ham\n",
    "                print(word, 1, 0)\n",
    "            else: # spam\n",
    "                print(word, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Reducer script for HW2.2. Aggregates results from mapper, emits summed counts of all words processed by mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce-2.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce-2.2.py\n",
    "#!/usr/bin/python\n",
    "'''\n",
    "    reducer aggregates counts of words from stdin and emits those counts\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "words = {}\n",
    "for line in sys.stdin:\n",
    "    tokens = line.split()\n",
    "    word = tokens[0]\n",
    "    if word not in words.keys():\n",
    "        words[word] = 0\n",
    "    # mapper produces counts based on email classification\n",
    "    # this reducer is only interested in total counts\n",
    "    words[word] += int(tokens[1]) + int(tokens[2])\n",
    "# emit results\n",
    "for word in sorted(words.keys()):\n",
    "    # ignore counts for email classification\n",
    "    if word != '__CLASS__':\n",
    "        #print('\\t'.join([word, str(words[word])]), file=sys.stderr)\n",
    "        print('\\t'.join([word, str(words[word])]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading enron email file\n",
      "preparing output directory: Deleted /out/out-2.2\n",
      "executing yarn task\n",
      "checking output directory: Found 2 items\n",
      "-rw-r--r--   1 david supergroup          0 2015-09-12 14:36 /out/out-2.2/_SUCCESS\n",
      "-rw-r--r--   1 david supergroup         14 2015-09-12 14:36 /out/out-2.2/part-00000\n",
      "result: assistance\t10\n"
     ]
    }
   ],
   "source": [
    "# HW2.2. Using the Enron data from HW1 and Hadoop MapReduce streaming, write \n",
    "# mapper/reducer pair that  will determine the number of occurrences of a \n",
    "# single, user-specified word.\n",
    "!printf \"loading enron email file\\n\"\n",
    "!hdfs dfs -put -f enronemail_1h.txt /in\n",
    "!printf \"preparing output directory: \"\n",
    "!hdfs dfs -rm -r -f -skipTrash /out/out-2.2\n",
    "!printf \"executing yarn task\\n\"\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \\\n",
    "    -files ./map.py,./reduce-2.2.py \\\n",
    "    -mapper './map.py \"assistance\"' \\\n",
    "    -reducer ./reduce-2.2.py \\\n",
    "    -input /in/enronemail_1h.txt -output /out/out-2.2\n",
    "!printf \"checking output directory: \"\n",
    "!hdfs dfs -ls /out/out-2.2\n",
    "!printf \"result: \"\n",
    "!hdfs dfs -cat /out/out-2.2/part-00000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Reducer script for HW2.3-5. Aggregates results from mapper to generate vocabulary and email classification counts.\n",
    "\n",
    "Test the results against the same data set, classifying email records and comparing the results to known classifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce-2.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce-2.3.py\n",
    "#!/usr/bin/python\n",
    "''' reducer aggregates counts of words and email classifications\n",
    "\n",
    "    reducer then applies a Naive Bayes classifier against the same data set as used\n",
    "    to buld the training parameters, classifying email records and comparing the\n",
    "    results to known classsifications.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "# store statistics on the original list of words of interest\n",
    "keywords = {}\n",
    "# counts of each email classification\n",
    "hamcount = 0\n",
    "spamcount = 0\n",
    "# counts of words in each email classification\n",
    "spamwordcount = 0\n",
    "hamwordcount = 0\n",
    "\n",
    "# minimum word frequency for inclusion\n",
    "minfrequency = sys.maxint\n",
    "try:\n",
    "    minfrequency = int(sys.argv[1])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tokens = line.split()\n",
    "    word = tokens[0]\n",
    "    # special case for count of email classification\n",
    "    if word == '__CLASS__':\n",
    "        hamcount += int(tokens[1])\n",
    "        spamcount += int(tokens[2])\n",
    "    # regular case of count of word\n",
    "    else:\n",
    "        if word not in keywords.keys():\n",
    "            keywords[word] = [0, 0]\n",
    "        keywords[word][0] += int(tokens[1])\n",
    "        keywords[word][1] += int(tokens[2])\n",
    "        hamwordcount += int(tokens[1])\n",
    "        spamwordcount += int(tokens[2])\n",
    "\n",
    "# prune low frequency words from vocabulary\n",
    "if minfrequency < sys.maxint:\n",
    "    tmpwords = {}\n",
    "    for word in keywords:\n",
    "        if sum(keywords[word]) < minfrequency:\n",
    "            # decrement word counts accordingly \n",
    "            hamwordcount -= keywords[word][0]\n",
    "            spamwordcount -= keywords[word][1]\n",
    "        else:\n",
    "            # keep high frequency words\n",
    "            tmpwords[word] = keywords[word]\n",
    "    print('prune: removed {} of {} total words due to frequency less than {}'\n",
    "          .format(len(keywords) - len(tmpwords), len(keywords), minfrequency), \n",
    "          file=sys.stderr)\n",
    "    keywords = tmpwords\n",
    "    \n",
    "# total number of unique words\n",
    "vocabcount = len(keywords)\n",
    "# total number of email records\n",
    "doccount = spamcount + hamcount\n",
    "\n",
    "# counters for determining error rate\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "# regular expression for removing punctuation\n",
    "punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "with open('enronemail_1h.txt', 'r') as cfile:\n",
    "    for line in cfile:\n",
    "        # words to be used in Naive Bayes classification\n",
    "        nbwords = {}\n",
    "        tokens = line.split('\\t', 2)\n",
    "        eid = tokens[0]\n",
    "        isspam = tokens[1]\n",
    "        # build bag of words for email record\n",
    "        text = tokens[len(tokens) - 1].lower()\n",
    "        text = punctuation.sub('', text)\n",
    "        docwords = re.findall(r\"\\w+\", text)\n",
    "        for word in docwords:\n",
    "            if word in keywords.keys():\n",
    "                if word not in nbwords:\n",
    "                    nbwords[word] = 1\n",
    "                else:\n",
    "                    nbwords[word] += 1\n",
    "\n",
    "        # calculate the probability of the email record being spam or ham\n",
    "        # natural log conversion is used to avoid floating point underflow\n",
    "\n",
    "        # start with the prior probability of a spam record\n",
    "        logpspam = math.log(spamcount / float(doccount))\n",
    "        for word in nbwords:\n",
    "            # add the probability of the word being present in this classification\n",
    "            # multiplied by the number of times the word appears in the record\n",
    "            logpspam += (nbwords[word] * \n",
    "                (math.log(keywords[word][1] + 1 / float(spamwordcount + vocabcount))))\n",
    "\n",
    "        # start with the prior probability of a ham record\n",
    "        logpham = math.log(hamcount / float(doccount))\n",
    "        for word in nbwords:\n",
    "            # add the probability of the word being present in this classification\n",
    "            # multiplied by the number of times the word appears in the record\n",
    "            logpham += (nbwords[word] * (math.log(keywords[word][0] + 1 / float(hamwordcount + vocabcount))))\n",
    "\n",
    "        # determine the classification, based on comparison of log probabilities\n",
    "        nbclass = '0' \n",
    "        if logpspam > logpham:\n",
    "            nbclass = '1'\n",
    "\n",
    "        # add some statistics\n",
    "        if isspam == nbclass:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "\n",
    "        # emit the results\n",
    "        #print('\\t'.join([eid, isspam, nbclass, str(isspam == nbclass)]), file=sys.stderr)\n",
    "        print('\\t'.join([eid, isspam, nbclass]))\n",
    "# print some statistics\n",
    "print('correct: {}, incorrect: {}, training error: {}'.format(correct, incorrect,\n",
    "    str(float(incorrect) / (correct + incorrect))), file=sys.stderr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading enron email file\n",
      "preparing output directory: Deleted /out/out-2.3\n",
      "executing yarn task\n",
      "correct: 60, incorrect: 40, training error: 0.4\n",
      "checking output directory: Found 2 items\n",
      "-rw-r--r--   1 david supergroup          0 2015-09-12 14:37 /out/out-2.3/_SUCCESS\n",
      "-rw-r--r--   1 david supergroup       2672 2015-09-12 14:37 /out/out-2.3/part-00000\n"
     ]
    }
   ],
   "source": [
    "# HW2.3. Using the Enron data from HW1 and Hadoop MapReduce, write  a \n",
    "# mapper/reducer pair that will classify the email messages by a single, \n",
    "# user-specified word.\n",
    "!printf \"loading enron email file\\n\"\n",
    "!hdfs dfs -put -f enronemail_1h.txt /in\n",
    "!printf \"preparing output directory: \"\n",
    "!hdfs dfs -rm -r -f -skipTrash /out/out-2.3\n",
    "!printf \"executing yarn task\\n\"\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \\\n",
    "    -files ./map.py,./reduce-2.3.py \\\n",
    "    -mapper './map.py \"assistance\"' \\\n",
    "    -reducer ./reduce-2.3.py \\\n",
    "    -input /in/enronemail_1h.txt -output /out/out-2.3\n",
    "!printf \"checking output directory: \"\n",
    "!hdfs dfs -ls /out/out-2.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading enron email file\n",
      "preparing output directory: Deleted /out/out-2.4\n",
      "executing yarn task\n",
      "correct: 63, incorrect: 37, training error: 0.37\n",
      "checking output directory: Found 2 items\n",
      "-rw-r--r--   1 david supergroup          0 2015-09-12 14:37 /out/out-2.4/_SUCCESS\n",
      "-rw-r--r--   1 david supergroup       2672 2015-09-12 14:37 /out/out-2.4/part-00000\n"
     ]
    }
   ],
   "source": [
    "# HW2.4. Using the Enron data from HW1 and in the Hadoop MapReduce framework, \n",
    "# write  a mapper/reducer pair that will classify the email messages using \n",
    "# multinomial Naive Bayes Classifier using a list of one or \n",
    "# more user-specified words.\n",
    "!printf \"loading enron email file\\n\"\n",
    "!hdfs dfs -put -f enronemail_1h.txt /in\n",
    "!printf \"preparing output directory: \"\n",
    "!hdfs dfs -rm -r -f -skipTrash /out/out-2.4\n",
    "!printf \"executing yarn task\\n\"\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \\\n",
    "    -files ./map.py,./reduce-2.3.py \\\n",
    "    -mapper './map.py \"assistance valium enlargementWithATypo\"' \\\n",
    "    -reducer ./reduce-2.3.py \\\n",
    "    -input /in/enronemail_1h.txt -output /out/out-2.4\n",
    "!printf \"checking output directory: \"\n",
    "!hdfs dfs -ls /out/out-2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### HW2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading enron email file\n",
      "preparing output directory: Deleted /out/out-2.5\n",
      "executing yarn task\n",
      "no minimum frequency for word inclusion in vocabulary\n",
      "correct: 100, incorrect: 0, training error: 0.0\n",
      "checking output directory: Found 2 items\n",
      "-rw-r--r--   1 david supergroup          0 2015-09-12 14:37 /out/out-2.5/_SUCCESS\n",
      "-rw-r--r--   1 david supergroup       2672 2015-09-12 14:37 /out/out-2.5/part-00000\n"
     ]
    }
   ],
   "source": [
    "# HW2.5. Using the Enron data from HW1 an in the  Hadoop MapReduce framework, \n",
    "# write  a mapper/reducer for a multinomial Naive Bayes Classifier that will \n",
    "# classify the email messages using  words present. Also drop words with a \n",
    "# frequency of less than three (3). How does it affect the misclassifcation \n",
    "# error of learnt naive multinomial Bayesian Classifiers on the training dataset\n",
    "!printf \"loading enron email file\\n\"\n",
    "!hdfs dfs -put -f enronemail_1h.txt /in\n",
    "!printf \"preparing output directory: \"\n",
    "!hdfs dfs -rm -r -f -skipTrash /out/out-2.5\n",
    "!printf \"executing yarn task\\n\"\n",
    "!printf \"no minimum frequency for word inclusion in vocabulary\\n\"\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \\\n",
    "    -files ./map.py,./reduce-2.3.py \\\n",
    "    -mapper './map.py \"*\"' \\\n",
    "    -reducer './reduce-2.3.py' \\\n",
    "    -input /in/enronemail_1h.txt -output /out/out-2.5\n",
    "!printf \"checking output directory: \"\n",
    "!hdfs dfs -ls /out/out-2.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing output directory: Deleted /out/out-2.5\n",
      "executing yarn task\n",
      "setting minimum frequency for word inclusion in vocabulary to 3\n",
      "prune: removed 3910 of 5739 total words due to frequency less than 3\n",
      "correct: 97, incorrect: 3, training error: 0.03\n",
      "checking output directory: Found 2 items\n",
      "-rw-r--r--   1 david supergroup          0 2015-09-12 14:37 /out/out-2.5/_SUCCESS\n",
      "-rw-r--r--   1 david supergroup       2672 2015-09-12 14:37 /out/out-2.5/part-00000\n"
     ]
    }
   ],
   "source": [
    "# HW2.5. ... Also drop words with a \n",
    "# frequency of less than three (3). How does it affect the misclassifcation \n",
    "# error of learnt naive multinomial Bayesian Classifiers on the training dataset.\n",
    "!printf \"preparing output directory: \"\n",
    "!hdfs dfs -rm -r -f -skipTrash /out/out-2.5\n",
    "!printf \"executing yarn task\\n\"\n",
    "!printf \"setting minimum frequency for word inclusion in vocabulary to 3\\n\"\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \\\n",
    "    -files ./map.py,./reduce-2.3.py \\\n",
    "    -mapper './map.py \"*\"' \\\n",
    "    -reducer './reduce-2.3.py 3' \\\n",
    "    -input /in/enronemail_1h.txt -output /out/out-2.5\n",
    "!printf \"checking output directory: \"\n",
    "!hdfs dfs -ls /out/out-2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: removing words with a frequency less than 3 results in an increase in training error from 0.0 to 0.03. This would indicate that some of those words, though uncommon, are strongly associated with one classification or the other, but not both, resulting in a more biased model when they are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "MIDS-W261-2015-HWK-Week01-rose.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
